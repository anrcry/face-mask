{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face Mask Detection",
      "provenance": [],
      "collapsed_sections": [
        "812X5DuyX7Ui"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdTm68Jb1f5f"
      },
      "source": [
        "# Classification of Mask - Final Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay7FQvb1_wMx"
      },
      "source": [
        "### Obtaining & Configuring the dataset (from Google Drive to Computer)\n",
        "\n",
        "> **PLEASE DO NOT RUN THIS HERE.**\n",
        "\n",
        "##### Steps:\n",
        "\n",
        "1. **Use:** `pip install google-api-python-client`.\n",
        "2. **Please refer to the Google Documentation on OAuth 2.0 & Drive API v3.**\n",
        "3. **Please enter:** `http://localhost:8080/` as a `redirect_uri` in the cloud console of Google for your Client ID & Secret Pair.\n",
        "4. **Please set up:** the OAuth 2.0 screen with one of the sensitive scopes as `./auth/drive` for full access to **Google Drive API**.\n",
        "\n",
        "\n",
        "\n",
        "After registering the client keys please do not forget to download and save `client_secrets.json`, at the same path as your program. That section is uploaded to [https://github.com/formula21/face-mask/blob/main/croppie.py](https://github.com/formula21/face-mask/blob/main/croppie.py).\n",
        "\n",
        "##### Other notes:\n",
        "\n",
        "1. The `imagebuffer` once stored as a file, stays on your computer, unless you define the `FLAG_UPLOAD = true` and `FLAG_UPLOAD_PARENT_ID = [None, None] or [str, str]`.\n",
        "  - If either `None` is defined,  a **new folder** with the names `with_mask` and `without_mask` are created or the id's are used to upload. If a failure of finding the directories in drive, the program terminates with an Exception.\n",
        "  - If either or both are found, we will send the buffer immediately to upload.\n",
        "2. By default, the file is supposed to be also saved locally, however this can be omitted by defining `FLAG_DOWNLOAD_AND_SAVE = false`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5WxAuji85tz"
      },
      "source": [
        "### Google Drive Authorization\n",
        "\n",
        "We are authorizing Google Colab with Google Drive for us to get access to our dataset. This authorization is native to Google's Documentation.\n",
        "\n",
        "**Please note:** You need to change the variable `dir` below to the appropriate path. If you are using a \"Shared With Me\" folder please set up a \"Add Shortcut to Drive\", to set shortcut and get easy access to &ldquo;My Drive&rdquo;."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vatYFF67r8i",
        "outputId": "1b344768-4b2a-432b-c61b-2010116fcb87"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        " \n",
        "# Mounting statement\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Directory (Please make appropiate changes)\n",
        "\n",
        "# Please make changes here.\n",
        "if not os.path.isdir('/content/models'):\n",
        "  os.mkdir('/content/models')\n",
        "\n",
        "if not os.path.isdir('/content/plot'):\n",
        "   os.mkdir('/content/plot')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "812X5DuyX7Ui"
      },
      "source": [
        "#### Directory\n",
        "\n",
        "We declare a variable `dir` independently, so that we can change the parent part as and when required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB9l5jPRIL2J"
      },
      "source": [
        "dir = '/content/drive/MyDrive/College/Sem 6/Project_FACE_MASK/fm_detector/dataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixauILmpJd_Z"
      },
      "source": [
        "## Section 1 ~ Convoluted Neural Networks\n",
        "\n",
        "We are introducing here 3 types of CNN to compare between their models.\n",
        "\n",
        "1. ResNet50\n",
        "2. DenseNet201\n",
        "3. MobilenetV2\n",
        "\n",
        "As most libraries are common, we try to import these all at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ml0x4hVYVF2"
      },
      "source": [
        "### Section 1.0: Pre Associating the images\n",
        "\n",
        "We import all images from the required dir, sort them according to labels and append them to a list. These images can be then passed to the `preprocess_input` function of the individual neural network libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJJKg6-hJY04"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "# from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Initial learning rate, can be extended\n",
        "intial_learning_rate = 1e-4\n",
        "\n",
        "# number of epochs to train\n",
        "epochs = 45 \n",
        "\n",
        "# The size of the batch to train on\n",
        "batch_size = 4\n",
        "\n",
        "categories = [\"with_mask\", \"without_mask\"]\n",
        "\n",
        "img_to_arr = []\n",
        "\n",
        "for category in categories:\n",
        "    path = os.path.join(dir, category)\n",
        "    x = os.listdir(path)\n",
        "    if category == \"without_mask\":\n",
        "       x = x[0:int(0.3 * len(x))]\n",
        "    for i in range(0, len(x)):\n",
        "        img = x[i]\n",
        "        path_img = os.path.join(path, img)\n",
        "        print(\"Loading %d out of %d in %s\" % ((i+1), len(x), category))\n",
        "        # see from tensorflow.keras.preprocessing.image import load_img\n",
        "        image = load_img(path_img, target_size=(224, 224))\n",
        "        # from tensorflow.keras.preprocessing.image import img_to_array\n",
        "        image = img_to_array(image)\n",
        "        # Appending globally to img_to_arr\n",
        "        img_to_arr.append({\"image\":image, \"category\":category})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wYBo-yj-EKX"
      },
      "source": [
        "### Section 1.1: ResNet 50\n",
        "\n",
        "> ResNet-50 is a convolutional neural network that is 50 layers deep. You can \n",
        "> load a pretrained version of the network trained on more than a million \n",
        "> images from the ImageNet database. The pretrained network can classify images \n",
        "> into 1000 object categories, such as keyboard, mouse, pencil, and many \n",
        "> animals.\n",
        "\n",
        "We have the documentation @ [https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcMa5FWNHa59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e0c6a70f-f88d-4b10-efa4-1d09148ddbff"
      },
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet50_preprocess_input\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "batch_size = 45\n",
        "intial_learning_rate = 0.003\n",
        "epoch = 20\n",
        "\n",
        "for img in img_to_arr:\n",
        "  image, category = img['image'], img['category']\n",
        "  # from tensorflow.keras.applications.* import preprocess_input\n",
        "  image = resnet50_preprocess_input(image)\n",
        "  # Append the image at index (x)\n",
        "  data.append(image)\n",
        "  # Append the label for each image... at an index (x)\n",
        "  labels.append(category)\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "labels = to_categorical(labels)\n",
        "\n",
        "# Setting the data (images) to pixels of float32\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "\n",
        "# Converting labels to array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Dataset split w.r.t ratio.\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "                                                  test_size=0.20, stratify=labels, random_state=42)\n",
        "# Data Augmentation\n",
        "# Extra sampling of images.\n",
        "aug = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\")\n",
        "\n",
        "resnet_model = ResNet50(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "resnet_head_model = resnet_model.output\n",
        "# 224 / 32 = 7\n",
        "# Window create -> Shifting\n",
        "resnet_head_model = AveragePooling2D(pool_size=(7, 7))(resnet_head_model)\n",
        "\n",
        "# Feature Map flatted to 1D Formation\n",
        "resnet_head_model = Flatten(name=\"flatten\")(resnet_head_model)\n",
        "\n",
        "# Action Function = RELU.. (Possibility)... Image Thresholding (Function making)\n",
        "# Rectified Linear Unit\n",
        "resnet_head_model = Dense(128, activation=\"relu\")(resnet_head_model)\n",
        "\n",
        "# Dropout -> 1/2 is expunged [It is a bias]\n",
        "resnet_head_model = Dropout(0.5)(resnet_head_model)\n",
        "\n",
        "# Max=[0, MAX] (Binary Classification)\n",
        "resnet_head_model = Dense(2, activation=\"softmax\")(resnet_head_model)\n",
        "\n",
        "\n",
        "model = Model(inputs=resnet_model.input, outputs=resnet_head_model)\n",
        "\n",
        "# Layers\n",
        "for layer in resnet_model.layers:\n",
        "    # Freezing so that a pre-set model at transfer learning ....\n",
        "    layer.trainable = False\n",
        "\n",
        "# TODO: Explaination needed?\n",
        "opt = Adam(lr=intial_learning_rate, decay=(intial_learning_rate / epochs))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# TODO: Explaination needed?\n",
        "resnet_head = model.fit(\n",
        "    aug.flow(trainX, trainY, batch_size=batch_size),\n",
        "    steps_per_epoch=len(trainX) // batch_size,\n",
        "    validation_data=(testX, testY),\n",
        "    validation_steps=len(testX) // batch_size,\n",
        "    epochs=epochs)\n",
        "\n",
        "# TODO: Explaination needed?\n",
        "predIdxs = model.predict(testX, batch_size=batch_size)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "# TODO: Explaination needed?\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "# TODO: Explaination needed?\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "                            target_names=lb.classes_))\n",
        "\n",
        "# Save the model\n",
        "# evaluate(), save()\n",
        "models_path = os.path.join(os.path.abspath(\"models\"), \"resnet.model\")\n",
        "model.save(models_path, save_format=\"h5\")\n",
        "\n",
        "N = epochs\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), resnet_head.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), resnet_head.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), resnet_head.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), resnet_head.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plot_path = os.path.join(os.path.abspath(\"plot\"), \"resnet.png\")\n",
        "plt.savefig(plot_path)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/45\n",
            "89/89 [==============================] - 88s 607ms/step - loss: 0.2975 - accuracy: 0.9107 - val_loss: 0.1674 - val_accuracy: 0.9535\n",
            "Epoch 2/45\n",
            "89/89 [==============================] - 53s 595ms/step - loss: 0.1653 - accuracy: 0.9495 - val_loss: 0.1114 - val_accuracy: 0.9594\n",
            "Epoch 3/45\n",
            "89/89 [==============================] - 53s 589ms/step - loss: 0.1618 - accuracy: 0.9472 - val_loss: 0.1083 - val_accuracy: 0.9683\n",
            "Epoch 4/45\n",
            "89/89 [==============================] - 53s 590ms/step - loss: 0.1525 - accuracy: 0.9525 - val_loss: 0.1139 - val_accuracy: 0.9604\n",
            "Epoch 5/45\n",
            "89/89 [==============================] - 52s 588ms/step - loss: 0.1289 - accuracy: 0.9562 - val_loss: 0.1198 - val_accuracy: 0.9624\n",
            "Epoch 6/45\n",
            "89/89 [==============================] - 52s 582ms/step - loss: 0.1432 - accuracy: 0.9537 - val_loss: 0.1085 - val_accuracy: 0.9634\n",
            "Epoch 7/45\n",
            "89/89 [==============================] - 52s 588ms/step - loss: 0.1444 - accuracy: 0.9537 - val_loss: 0.1243 - val_accuracy: 0.9644\n",
            "Epoch 8/45\n",
            "89/89 [==============================] - 54s 608ms/step - loss: 0.1197 - accuracy: 0.9615 - val_loss: 0.1226 - val_accuracy: 0.9634\n",
            "Epoch 9/45\n",
            "89/89 [==============================] - 52s 586ms/step - loss: 0.1240 - accuracy: 0.9590 - val_loss: 0.1091 - val_accuracy: 0.9634\n",
            "Epoch 10/45\n",
            "89/89 [==============================] - 52s 584ms/step - loss: 0.1228 - accuracy: 0.9607 - val_loss: 0.1247 - val_accuracy: 0.9644\n",
            "Epoch 11/45\n",
            "89/89 [==============================] - 52s 579ms/step - loss: 0.1235 - accuracy: 0.9550 - val_loss: 0.1131 - val_accuracy: 0.9683\n",
            "Epoch 12/45\n",
            "89/89 [==============================] - 52s 584ms/step - loss: 0.1120 - accuracy: 0.9620 - val_loss: 0.1056 - val_accuracy: 0.9683\n",
            "Epoch 13/45\n",
            "89/89 [==============================] - 52s 585ms/step - loss: 0.1152 - accuracy: 0.9632 - val_loss: 0.1191 - val_accuracy: 0.9664\n",
            "Epoch 14/45\n",
            "89/89 [==============================] - 53s 589ms/step - loss: 0.1219 - accuracy: 0.9597 - val_loss: 0.1201 - val_accuracy: 0.9664\n",
            "Epoch 15/45\n",
            "89/89 [==============================] - 52s 585ms/step - loss: 0.1126 - accuracy: 0.9620 - val_loss: 0.1162 - val_accuracy: 0.9674\n",
            "Epoch 16/45\n",
            "89/89 [==============================] - 52s 585ms/step - loss: 0.1109 - accuracy: 0.9630 - val_loss: 0.1178 - val_accuracy: 0.9634\n",
            "Epoch 17/45\n",
            "89/89 [==============================] - 52s 583ms/step - loss: 0.1124 - accuracy: 0.9642 - val_loss: 0.1149 - val_accuracy: 0.9644\n",
            "Epoch 18/45\n",
            "89/89 [==============================] - 52s 579ms/step - loss: 0.1012 - accuracy: 0.9657 - val_loss: 0.1248 - val_accuracy: 0.9624\n",
            "Epoch 19/45\n",
            "89/89 [==============================] - 51s 567ms/step - loss: 0.1045 - accuracy: 0.9635 - val_loss: 0.1195 - val_accuracy: 0.9634\n",
            "Epoch 20/45\n",
            "89/89 [==============================] - 51s 570ms/step - loss: 0.1153 - accuracy: 0.9642 - val_loss: 0.1223 - val_accuracy: 0.9644\n",
            "Epoch 21/45\n",
            "89/89 [==============================] - 50s 566ms/step - loss: 0.1035 - accuracy: 0.9635 - val_loss: 0.1190 - val_accuracy: 0.9644\n",
            "Epoch 22/45\n",
            "89/89 [==============================] - 51s 568ms/step - loss: 0.1013 - accuracy: 0.9650 - val_loss: 0.1190 - val_accuracy: 0.9693\n",
            "Epoch 23/45\n",
            "89/89 [==============================] - 51s 573ms/step - loss: 0.1098 - accuracy: 0.9633 - val_loss: 0.1241 - val_accuracy: 0.9693\n",
            "Epoch 24/45\n",
            "89/89 [==============================] - 50s 566ms/step - loss: 0.0981 - accuracy: 0.9667 - val_loss: 0.1286 - val_accuracy: 0.9674\n",
            "Epoch 25/45\n",
            "89/89 [==============================] - 50s 566ms/step - loss: 0.0982 - accuracy: 0.9652 - val_loss: 0.1322 - val_accuracy: 0.9654\n",
            "Epoch 26/45\n",
            "89/89 [==============================] - 51s 568ms/step - loss: 0.0987 - accuracy: 0.9672 - val_loss: 0.1308 - val_accuracy: 0.9634\n",
            "Epoch 27/45\n",
            "89/89 [==============================] - 51s 570ms/step - loss: 0.1049 - accuracy: 0.9637 - val_loss: 0.1408 - val_accuracy: 0.9644\n",
            "Epoch 28/45\n",
            "89/89 [==============================] - 51s 574ms/step - loss: 0.0964 - accuracy: 0.9657 - val_loss: 0.1346 - val_accuracy: 0.9624\n",
            "Epoch 29/45\n",
            "89/89 [==============================] - 51s 574ms/step - loss: 0.0843 - accuracy: 0.9712 - val_loss: 0.1473 - val_accuracy: 0.9614\n",
            "Epoch 30/45\n",
            "89/89 [==============================] - 51s 575ms/step - loss: 0.0825 - accuracy: 0.9687 - val_loss: 0.1525 - val_accuracy: 0.9683\n",
            "Epoch 31/45\n",
            "89/89 [==============================] - 51s 577ms/step - loss: 0.1023 - accuracy: 0.9655 - val_loss: 0.1339 - val_accuracy: 0.9664\n",
            "Epoch 32/45\n",
            "89/89 [==============================] - 51s 576ms/step - loss: 0.0889 - accuracy: 0.9687 - val_loss: 0.1323 - val_accuracy: 0.9654\n",
            "Epoch 33/45\n",
            "89/89 [==============================] - 52s 580ms/step - loss: 0.0967 - accuracy: 0.9670 - val_loss: 0.1361 - val_accuracy: 0.9683\n",
            "Epoch 34/45\n",
            "89/89 [==============================] - 51s 575ms/step - loss: 0.0893 - accuracy: 0.9677 - val_loss: 0.1388 - val_accuracy: 0.9664\n",
            "Epoch 35/45\n",
            "89/89 [==============================] - 52s 579ms/step - loss: 0.0966 - accuracy: 0.9662 - val_loss: 0.1369 - val_accuracy: 0.9644\n",
            "Epoch 36/45\n",
            "89/89 [==============================] - 53s 590ms/step - loss: 0.0852 - accuracy: 0.9670 - val_loss: 0.1228 - val_accuracy: 0.9624\n",
            "Epoch 37/45\n",
            "89/89 [==============================] - 53s 599ms/step - loss: 0.0926 - accuracy: 0.9685 - val_loss: 0.1339 - val_accuracy: 0.9644\n",
            "Epoch 38/45\n",
            "89/89 [==============================] - 53s 592ms/step - loss: 0.0852 - accuracy: 0.9735 - val_loss: 0.1288 - val_accuracy: 0.9614\n",
            "Epoch 39/45\n",
            "89/89 [==============================] - 55s 616ms/step - loss: 0.0837 - accuracy: 0.9692 - val_loss: 0.1340 - val_accuracy: 0.9664\n",
            "Epoch 40/45\n",
            "89/89 [==============================] - 53s 595ms/step - loss: 0.0919 - accuracy: 0.9670 - val_loss: 0.1215 - val_accuracy: 0.9674\n",
            "Epoch 41/45\n",
            "89/89 [==============================] - 53s 593ms/step - loss: 0.0863 - accuracy: 0.9727 - val_loss: 0.1237 - val_accuracy: 0.9693\n",
            "Epoch 42/45\n",
            "89/89 [==============================] - 53s 593ms/step - loss: 0.0813 - accuracy: 0.9727 - val_loss: 0.1257 - val_accuracy: 0.9674\n",
            "Epoch 43/45\n",
            "89/89 [==============================] - 53s 592ms/step - loss: 0.0752 - accuracy: 0.9717 - val_loss: 0.1574 - val_accuracy: 0.9683\n",
            "Epoch 44/45\n",
            "89/89 [==============================] - 55s 616ms/step - loss: 0.0917 - accuracy: 0.9695 - val_loss: 0.1245 - val_accuracy: 0.9674\n",
            "Epoch 45/45\n",
            "89/89 [==============================] - 53s 592ms/step - loss: 0.0773 - accuracy: 0.9720 - val_loss: 0.1345 - val_accuracy: 0.9654\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   with_mask       0.96      0.95      0.95       377\n",
            "without_mask       0.97      0.98      0.97       634\n",
            "\n",
            "    accuracy                           0.97      1011\n",
            "   macro avg       0.96      0.96      0.96      1011\n",
            "weighted avg       0.97      0.97      0.97      1011\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_LhaA4cMdPz"
      },
      "source": [
        "### Section 1.2: DenseNet 201\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm47eANIMtBo"
      },
      "source": [
        "from tensorflow.keras.applications import DenseNet201\n",
        "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess_input\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# Initial learning rate, can be extended\n",
        "intial_learning_rate = 1e-4\n",
        "\n",
        "# number of epochs to train\n",
        "epochs = 10\n",
        "\n",
        "batch_size = 18\n",
        "\n",
        "for img in img_to_arr:\n",
        "  image, category = img['image'], img['category']\n",
        "  # from tensorflow.keras.applications.* import preprocess_input\n",
        "  image = densenet_preprocess_input(image)\n",
        "  # Append the image at index (x)\n",
        "  data.append(image)\n",
        "  # Append the label for each image... at an index (x)\n",
        "  labels.append(category)\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "labels = to_categorical(labels)\n",
        "\n",
        "# Setting the data (images) to pixels of float32\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "\n",
        "# Converting labels to array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Dataset split w.r.t ratio.\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "                                                  test_size=0.20, stratify=labels, random_state=42)\n",
        "# Data Augmentation\n",
        "# Extra sampling of images.\n",
        "aug = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\")\n",
        "\n",
        "dense_model = DenseNet201(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "dense_head_model = dense_model.output\n",
        "# 224 / 32 = 7\n",
        "# Window create -> Shifting\n",
        "dense_head_model = AveragePooling2D(pool_size=(7, 7))(dense_head_model)\n",
        "\n",
        "# Feature Map flatted to 1D Formation\n",
        "dense_head_model = Flatten(name=\"flatten\")(dense_head_model)\n",
        "\n",
        "# Action Function = RELU.. (Possibility)... Image Thresholding (Function making)\n",
        "# Rectified Linear Unit\n",
        "dense_head_model = Dense(128, activation=\"relu\")(dense_head_model)\n",
        "\n",
        "# Dropout -> 1/2 is expunged [It is a bias]\n",
        "dense_head_model = Dropout(0.5)(dense_head_model)\n",
        "\n",
        "# Max=[0, MAX] (Binary Classification)\n",
        "dense_head_model = Dense(2, activation=\"softmax\")(dense_head_model)\n",
        "\n",
        "\n",
        "model = Model(inputs=dense_model.input, outputs=dense_head_model)\n",
        "\n",
        "# Layers\n",
        "for layer in dense_model.layers:\n",
        "    # Freezing so that a pre-set model at transfer learning ....\n",
        "    layer.trainable = False\n",
        "\n",
        "# TODO: Explaination needed?\n",
        "opt = Adam(lr=intial_learning_rate, decay=(intial_learning_rate / epochs))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# TODO: Explaination needed?\n",
        "dense_head = model.fit(\n",
        "    aug.flow(trainX, trainY, batch_size=batch_size),\n",
        "    steps_per_epoch=len(trainX) // batch_size,\n",
        "    validation_data=(testX, testY),\n",
        "    validation_steps=len(testX) // batch_size,\n",
        "    epochs=epochs)\n",
        "\n",
        "# TODO: Explaination needed?\n",
        "predIdxs = model.predict(testX, batch_size=batch_size)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "# TODO: Explaination needed?\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "# TODO: Explaination needed?\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "                            target_names=lb.classes_))\n",
        "\n",
        "# Save the model\n",
        "# evaluate(), save()\n",
        "models_path = os.path.join(os.path.abspath(\"models\"), \"densenet.model\")\n",
        "model.save(models_path, save_format=\"h5\")\n",
        "\n",
        "N = epochs\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), dense_head.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), dense_head.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), dense_head.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), dense_head.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plot_path = os.path.join(os.path.abspath(\"plot\"), \"densenet.png\")\n",
        "plt.savefig(plot_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0H-WlccD5Jq"
      },
      "source": [
        "### Section 1.3: MobileNet v2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzLmPWoUEOmF"
      },
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_v2_preprocess_input\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# Initial learning rate, can be extended\n",
        "intial_learning_rate = 1e-4\n",
        "\n",
        "# number of epochs to train\n",
        "epochs = 45\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "for img in img_to_arr:\n",
        "  image, category = img['image'], img['category']\n",
        "  # from tensorflow.keras.applications.* import preprocess_input\n",
        "  image = mobilenet_v2_preprocess_input(image)\n",
        "  # Append the image at index (x)\n",
        "  data.append(image)\n",
        "  # Append the label for each image... at an index (x)\n",
        "  labels.append(category)\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "labels = to_categorical(labels)\n",
        "\n",
        "# Setting the data (images) to pixels of float32\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "\n",
        "# Converting labels to array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Dataset split w.r.t ratio.\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "                                                  test_size=0.20, stratify=labels, random_state=42)\n",
        "# Data Augmentation\n",
        "# Extra sampling of images.\n",
        "aug = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\")\n",
        "\n",
        "mobile_model = MobileNetV2(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "mobile_head_model = mobile_model.output\n",
        "# 224 / 32 = 7\n",
        "# Window create -> Shifting\n",
        "mobile_head_model = AveragePooling2D(pool_size=(7, 7))(mobile_head_model)\n",
        "\n",
        "# Feature Map flatted to 1D Formation\n",
        "mobile_head_model = Flatten(name=\"flatten\")(mobile_head_model)\n",
        "\n",
        "# Action Function = RELU.. (Possibility)... Image Thresholding (Function making)\n",
        "# Rectified Linear Unit\n",
        "mobile_head_model = Dense(128, activation=\"relu\")(mobile_head_model)\n",
        "\n",
        "# Dropout -> 1/2 is expunged [It is a bias]\n",
        "mobile_head_model = Dropout(0.5)(mobile_head_model)\n",
        "\n",
        "# Max=[0, MAX] (Binary Classification)\n",
        "mobile_head_model = Dense(2, activation=\"softmax\")(mobile_head_model)\n",
        "\n",
        "\n",
        "model = Model(inputs=mobile_model.input, outputs=mobile_head_model)\n",
        "\n",
        "# Layers\n",
        "for layer in mobile_model.layers:\n",
        "    # Freezing so that a pre-set model at transfer learning ....\n",
        "    layer.trainable = False\n",
        "\n",
        "# TODO: Explaination needed?\n",
        "opt = Adam(lr=intial_learning_rate, decay=(intial_learning_rate / epochs))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# TODO: Explaination needed?\n",
        "mobile_head = model.fit(\n",
        "    aug.flow(trainX, trainY, batch_size=batch_size),\n",
        "    steps_per_epoch=len(trainX) // batch_size,\n",
        "    validation_data=(testX, testY),\n",
        "    validation_steps=len(testX) // batch_size,\n",
        "    epochs=epochs)\n",
        "\n",
        "# TODO: Explaination needed?\n",
        "predIdxs = model.predict(testX, batch_size=batch_size)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "# TODO: Explaination needed?\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "# TODO: Explaination needed?\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "                            target_names=lb.classes_))\n",
        "\n",
        "# Save the model\n",
        "# evaluate(), save()\n",
        "models_path = os.path.join(os.path.abspath(\"models\"), \"mobilenet.model\")\n",
        "model.save(models_path, save_format=\"h5\")\n",
        "\n",
        "N = epochs\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), mobile_head.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), mobile_head.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), mobile_head.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), mobile_head.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plot_path = os.path.join(os.path.abspath(\"plot\"), \"mobilenet.png\")\n",
        "plt.savefig(plot_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}